# -*- coding: utf-8 -*-
"""Annu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n2D9TuflMjdotE9ObaKvI4QNHFgbsnRH
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.pyplot as plt
import matplotlib
# %matplotlib inline
sns.set_style('darkgrid')

from datetime import date

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV
from sklearn.impute import KNNImputer

from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing, model_selection
from sklearn.model_selection import RepeatedStratifiedKFold

from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.impute import KNNImputer,SimpleImputer

from google.colab import drive
drive.mount('/content/drive')

#!ls /content/drive/MyDrive/Technocolabs_Project/Data-Analyst-Internship-Project-Code-TCSDA45689/data/LoanExport.csv

LoanE = pd.read_csv("/content/drive/MyDrive/Internship Project Team/Dataset/LoanExport.csv")

#LoanE = pd.read_csv('/Users/nisha/TechnoColabs/Data-Analyst-Internship-Project-Code-TCSDA45689/data/LoanExport.csv')

LoanE.head(10)

LoanE.info()

"""****"""

LoanE.shape

LoanE.columns

LoanE["FirstPaymentDate"] = pd.to_datetime(LoanE["FirstPaymentDate"],format="%Y%m")
LoanE["MaturityDate"] = pd.to_datetime(LoanE["MaturityDate"],format="%Y%m")

LoanE["FirstPaymentDate"].head()

"""#### Checking for:
Data types . Presence of nulls
"""

LoanE.isnull().sum()

"""In LoanExport Dataset Seller Name is missing

### Data Exploration
"""

#list of all the numeric columns
num = LoanE.select_dtypes('number').columns.to_list()

#list of all the categoric columns
cat = LoanE.select_dtypes('object').columns.to_list()

#numeric df
Loan_num =  LoanE[num]

#categoric df
Loan_cat = LoanE[cat]

print(num)
print(cat)

#LoanE['MSA'].nunique()
#LoanE['MSA'].value_counts()
#Str_MSA= LoanE['MSA'].str.strip()
#Str_MSA_New = Str_MSA.apply(lambda v: '0' if v == 'X' else v)
#Str_MSA_New= Str_MSA_New.apply(int)
#Str_MSA_New= Str_MSA_New.apply(int)
#Str_MSA_New.head(10)

#Checking For Outliers
plt.figure(figsize=(8,5))
sns.boxplot('MonthsDelinquent', data=LoanE)

"""There are no outliers in Months Delinquent"""

plt.figure(figsize=(8,5))
sns.boxplot('MonthsInRepayment', data=LoanE)

plt.figure(figsize=(8,5))
sns.boxplot('OrigUPB', data=LoanE)

"""### Explore Metropolitan Statistical Area (MSA)
 It may be divided into smaller groups of counties that the United States Office 
of Management and Budget refers to as Metropolitan Divisions.

values  missing because the area is not a metropolitan area or the value is unknown. 
Since, we are not sure what the value could be, we will fill the missing values with Unknown.
"""

for numeric in Loan_num[num[:3]]:
    plt.scatter(Loan_num[numeric], Loan_num['MonthsInRepayment'])
    plt.title(numeric)
    plt.ylabel('MonthsInRepayment')
    plt.show()

"""MIP(Mortgage Insurance percentage) - The data is very spreaded, no specific pattern.

Units- Months in Repayments are same for every units.
"""

numeric = LoanE.select_dtypes('number').columns.to_list()
numeric 
BM_num =  LoanE[num]

plt.hist(LoanE['CreditScore'])
plt.title("CreditScore")
plt.show()

"""Credit Score- Most of the people have Credit Score between 700 to 800."""

sns.countplot('FirstTimeHomebuyer', data=LoanE)

"""Majority of the borrowers are not first-time home buyers."""

sns.countplot('PPM', data=LoanE)

"""Most of the people doesn't have Prepayment Penalty Mortgage."""

LoanE['EverDelinquent'].value_counts()

sns.countplot('EverDelinquent', data=LoanE)

"""19.78% Applications has defaulted atleast 1 time """

LoanE['NumBorrowers'].value_counts()

LoanE.iat[1256,22]

sns.countplot('NumBorrowers', data=LoanE)

"""Most of it are two borrowers."""

LoanE.PropertyType.value_counts()

PropertyTypes=LoanE.PropertyType.value_counts().head(5)
sns.barplot(PropertyTypes.index,PropertyTypes)
plt.xticks(rotation=90)
plt.figure(figsize=(15,15));

"""CO: Condo

PU: Planned Unit Development

MH: Manufactured Housing

SF: Single-Family

CP: Cooperative share

LH: Leasehold

X: Missing Values

Observations:

Single family homes are the most popular option amongst borrowers by a large margin.

22(0.1%) data are missing.
"""

plt.figure(figsize=(25,7))
sns.countplot('SellerName', data=LoanE)

LoanE.SellerName.value_counts()

"""Seller name 'ot' is the highest and then seller 'co'"""

plt.figure(figsize=(35,18))
sns.countplot('Channel', data=LoanE)

""" T- Third party
 
 R- Retail
 
 C- Correspondence
 
 B- Brocker

 
 Third party channel is the highest and then Retail.
"""

plt.figure(figsize=(35,18))
sns.countplot('ServicerName', data=LoanE)

"""Other servicers are more and then Countrywide is more in number."""

LoanPurposes=LoanE.LoanPurpose.value_counts().head(5)
sns.barplot(LoanPurposes.index,LoanPurposes)
plt.xticks(rotation=90)
plt.figure(figsize=(15,15));

"""P: Purchase

C: Refinance - Cash Out

N: Refinance - No Cash Out

Majority of the loans requested are for purchasing a property.
"""

plt.figure(figsize=(10,5))
sns.barplot('EverDelinquent' ,'CreditScore', data= LoanE ,palette='YlOrRd')
plt.xlabel('EverDelinquent', fontsize=14)
plt.legend()
plt.show()

"""From the above plot we can see that as credit score increases the likelihood of defaulting on the loan decreases. 
This is expected. A higher credit score indicates the borrower is relatively more capable of paying off the loan.
For credit score 500-690, the likelihood of defaulting on a loan is higher.
For credit score >690, the likelihood of not defaulting on the loan is higher.

"""

LoanE.columns

#plotting the correlation matrix

sns.heatmap(LoanE.corr() ,cmap='rocket')

"""OCLTV is highly correlated with LTV

### Feature Engineering
"""

#Function for replace the not available values to "nan"
def feature_nan(LoanE):
    LoanE.replace('X', np.nan, inplace = True)
   #For feature property type 
    LoanE.replace('X ', np.nan, inplace = True)
    #For MSA
    LoanE.replace('X    ', np.nan, inplace = True)    
    LoanE['CreditScore'].replace(0, np.nan, inplace= True)
    LoanE['LTV'].replace(0, np.nan, inplace= True)
    LoanE['Units'].replace(0, np.nan, inplace= True)
    return(LoanE)

#function for converting MSA features to [1,0]
def feature_msa(LoanE):
    LoanE['MSA'].iloc[LoanE['MSA'].notnull()] = 1
    LoanE['MSA'].fillna(0, inplace= True)
    return(LoanE)

LoanE= feature_nan(LoanE)
LoanE = feature_msa(LoanE)

LoanE.isnull().sum()

LoanE.isnull().sum()/LoanE.shape[0]*100

plt.figure(figsize=(8,5))
sns.boxplot('CreditScore', data=LoanE)

LoanE =  LoanE.dropna(subset=['PPM'])

Q1= LoanE['CreditScore'].quantile(0.25)
Q3 = LoanE['CreditScore'].quantile(0.75)
IQR = Q3 - Q1
Higher = Q3 + 1.5 * IQR
lower = Q1 - 1.5 * IQR
LoanE=LoanE[(LoanE['CreditScore'] > lower) & (LoanE['CreditScore'] < Higher)]
fig, axes = plt.subplots(1,2,figsize=(10,5))
sns.distplot(LoanE['CreditScore'],color='m',ax=axes[0])
sns.boxplot(y=LoanE['CreditScore'],color='m',ax=axes[1])
axes[0].set_title('Distribution of CreditScore without outliers')
axes[1].set_title('Box plot of CreditScore')
plt.tight_layout()
plt.show()
print(LoanE.shape)

LoanE.isnull().sum()/LoanE.shape[0]*100

plt.figure(figsize=(8,5))
sns.boxplot('CreditScore', data=LoanE)

LoanE.shape

LoanE.Units.value_counts()

LoanE.isna().sum()

LoanE =  LoanE.dropna(subset=['PPM'])

"""Assumption:-
Sellar name 'NA' might be Servicer name 'NATLCITYMTGECO'
But Panda taking NA as a null value which might not that's why will replace this with 'NT'
"""

LoanE['SellerName'].fillna('NT',inplace = True)

LoanE.isna().sum()

LoanE.shape

"""#### Dropping column which is not useful"""

LoanD= LoanE.drop(['FirstPaymentDate','MaturityDate','OCLTV','PostalCode','LoanSeqNum','ProductType','PropertyState'],axis= 1)

LoanD.head(10)

LoanD.shape

"""Label Encoding"""

#label encoding

le = LabelEncoder()
Label = ['PPM']
for i in Label:
    LoanD[i] = le.fit_transform(LoanD[i])

LoanD.shape

LoanD['PPM'].head()

#list of all the categoric columns
cate = LoanD.select_dtypes('object').columns.to_list()

#categoric df
LoanD_cate = LoanE[cate]

cate

"""#one hot encoding"""

# Apply one-hot encoder
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
LoanD_oh = pd.DataFrame(OH_encoder.fit_transform(LoanD[cate])).astype('int64')

#get feature columns
LoanD_oh.columns = OH_encoder.get_feature_names(cate)

# One-hot encoding removed index; put it back
LoanD_oh.index = LoanD.index
# LoanD1 = LoanD.drop(cols)

# Add one-hot encoded columns 
LoanD_fe = pd.concat([LoanD, LoanD_oh], axis=1)

LoanD.columns

LoanD_fe.head()

#LoanD_fe.columns

LoanD_fe.shape

LoanD_fe.drop(cate, axis=1, inplace=True)

LoanD_fe.shape

"""Imputing null values with KNN Imputer"""

imputer = KNNImputer(n_neighbors=2)
LoanD_fe_arr = imputer.fit_transform(LoanD_fe)
LoanD_fe = pd.DataFrame(LoanD_fe_arr, columns=LoanD_fe.columns)

"""Splitting into test and train data"""

from sklearn.model_selection import StratifiedShuffleSplit
y = LoanD_fe['PPM']
X = LoanD_fe.drop('PPM', axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify= y)

X_train.head(5)

y.value_counts()

y_test.value_counts()

y_train.value_counts()

X_train.dtypes

"""**Building** **Models**

"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""Logistic Regression"""

model= LogisticRegression(solver='newton-cg', class_weight='balanced')
model.fit(X_train,y_train)
model.score(X_test, y_test)

from sklearn.metrics import classification_report
preds = model.predict(X_test)
print(classification_report(y_test, preds))

from sklearn.metrics import classification_report
preds = model.predict(X_train)
print(classification_report(y_train, preds))

"""### Now, let us try to do Hyper-parameter Tunning using Cross-Validation via Grid Search"""

# parameter grid
parameters = {
    'penalty' : ['l1','l2'], 
    'C'       : np.logspace(-3,3,7),
    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],
}

logreg = LogisticRegression(class_weight='balanced')
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
clf = GridSearchCV(logreg,                    # model
                   param_grid = parameters,   # hyperparameters
                   scoring='accuracy',        # metric for scoring
                   cv=cv)                     # number of folds

clf.fit(X_train,y_train)

clf.score(X_test, y_test)

from sklearn.metrics import classification_report
preds = clf.predict(X_test)
print(classification_report(y_test, preds))

"""K-Nearest Neighbors"""

#Fitting K-NN classifier to the training set  
from sklearn.neighbors import KNeighborsClassifier  
classifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  
classifier.fit(X_train, y_train)

from sklearn.metrics import classification_report
preds = classifier.predict(X_test)
print(classification_report(y_test, preds))

"""We have a huge Class Imbalance!!"""

import imblearn

def get_model(model_name):
    if model_name == "decision_tree":
        model = DecisionTreeClassifier(class_weight="balanced", random_state=0)
    elif model_name == "random_forest":
        model = RandomForestClassifier(class_weight="balanced", random_state=0)
    elif model_name == "logistic_regression":
        model = LogisticRegression(class_weight="balanced", max_iter=10000, random_state=0, C=0.001, solver='liblinear', penalty='l1')
    else:
        raise ValueError("model_name should be one of \"decision_tree\" or \"random_forest\" or \"logistic_regression\"")
    return model

"""Treating class imbalance using Synthetic Minority Oversampling Technique (SMOTE)"""

def create_smote_pipeline(model_name):
    # categoric_transformer = ColumnTransformer([("cat", OneHotEncoder(sparse=False), categoric_cols)], remainder="passthrough")
    over_sampler = imblearn.over_sampling.SMOTE(sampling_strategy=0.25, random_state=42)
    under_sampler = imblearn.under_sampling.RandomUnderSampler(sampling_strategy=0.25, random_state=42)
    model = get_model(model_name)
    
    smote_model_pipeline = imblearn.pipeline.Pipeline(
        steps=[
            # ("categoric_transformer", categoric_transformer),
            ("minority_over_sampler", over_sampler),
            ("majority_under_sampler", under_sampler),
            ("model", model)
        ]
    )
    return smote_model_pipeline

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc

def get_train_test_scores(model_pipeline):
    model_pipeline.fit(X_train, y_train) 
    
    train_predictions = model_pipeline.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_predictions)

    fpr, tpr, thresholds = roc_curve(y_train, train_predictions)
    roc_auc = auc(fpr, tpr)
    print("Area under the ROC curve : %f" % roc_auc)

    ####################################
    # The optimal cut off would be where tpr is high and fpr is low
    # tpr - (1-fpr) is zero or near to zero is the optimal cut off point
    ####################################
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    print(f"optimal threshold for prediction: {optimal_threshold}")
    
    test_predictions = model_pipeline.predict(X_test)
    # print(test_predictions[:10])
    # test_predictions = [1 if pred > 0.75 else 0 for pred in test_predictions]
    test_accuracy = accuracy_score(y_test, test_predictions)
    
    confusion_scores_df = pd.DataFrame(confusion_matrix(y_test, test_predictions))
    classification_rep = classification_report(y_test, test_predictions)
    
    print(f"train_accuracy: {train_accuracy}")
    print(f"test_accuracy: {test_accuracy}")
    print(f"confusion_matrix:\n{confusion_scores_df}")
    print(f"classification_report:\n{classification_rep}")

smote_dtree_pipeline = create_smote_pipeline("logistic_regression")
get_train_test_scores(smote_dtree_pipeline)